# Log Central local stack: Vector → Kafka (KRaft) → Spark (to S3/Athena),
# Spring Boot Orchestrator + MySQL, Grafana, and Cron jobs.

name: log-central

networks:
  lc_net:
    name: lc_net

volumes:
  kafka_data: {}
  zookeeper_data: {}
  mysql_data: {}
  grafana_data: {}
  spark_checkpoint: {}
  spark_logs: {}
  spark_master_logs: {}
  spark_worker_logs: {}
  db_data: {}
  healthcheck_logs: {}
  otel_storage: {}

services:
  vector:
    image: timberio/vector:0.45.0-alpine
    container_name: lc_vector
    env_file:
      - .env
    environment:
      # Vector → Kafka parameters (provided in .env)
      - KAFKA_BROKERS=${KAFKA_BROKERS}
      # Enable Vector internal API (not exposed outside network)
      - VECTOR_API_VERSION=2
      - VECTOR_API_ADDRESS=0.0.0.0:8686
      - VECTOR_WATCH_CONFIG=true
    volumes:
      - ./vector/vector.yaml:/etc/vector/vector.yaml:ro
      - ./vector/logcentral_logs.desc:/etc/vector/logwise-vector.desc:ro

    expose:
      - "8686" # internal-only Vector API
      - "4317" # OTLP gRPC
      - "4318" # OTLP HTTP
    networks:
      - lc_net
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "sh", "-lc", "curl -fsS http://127.0.0.1:8686/health || vector --version"]
      interval: 15s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    container_name: lc_zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: srvr,ruok,stat,conf
    volumes:
      - zookeeper_data:/var/lib/zookeeper
    expose:
      - "2181"
    networks:
      - lc_net
    healthcheck:
      test: ["CMD", "bash", "-lc", "zookeeper-shell localhost:2181 ls / >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 10s
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: lc_kafka
    env_file:
      - .env
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      # Enable JMX for Kafka Manager polling
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: kafka
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=9999 -Djava.rmi.server.hostname=kafka
    volumes:
      - kafka_data:/var/lib/kafka/data
    expose:
      - "9092"
      - "9999"  # JMX port for Kafka Manager polling
    networks:
      - lc_net
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-lc", "kafka-topics --bootstrap-server localhost:9092 --list >/dev/null 2>&1"]
      interval: 15s
      timeout: 10s
      retries: 20
      start_period: 20s
    restart: unless-stopped


  db:
    image: mysql:8.0
    container_name: lc_mysql
    command: >
      --default-authentication-plugin=mysql_native_password
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_0900_ai_ci
    environment:
      MYSQL_DATABASE: ${MYSQL_DATABASE:-myapp}
      MYSQL_USER: ${MYSQL_USER:-myapp}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-myapp_pass}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root_pass}
      DB_HOST: db
    ports:
      - "3306:3306"
    networks:
      - lc_net
    volumes:
      - db_data:/var/lib/mysql
      - ./db/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-uroot", "-p${MYSQL_ROOT_PASSWORD:-root_pass}"]
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped
    

  orchestrator:
    # Uses existing Dockerfile in module: log-central-orchestror
    build:
      context: ./log-central-orchestrator
      dockerfile: Dockerfile
    container_name: lc_orchestrator
    env_file:
      - .env
    environment:
      # Database envs consumed by application.yml placeholders
      - DB_NAME=${MYSQL_DATABASE:-myapp}
      - DB_USER=${MYSQL_USER:-myapp}
      - DB_PASS=${MYSQL_PASSWORD:-myapp_pass}
      - MYSQL_ROOT_PASSWORD=${MYSQL_PASSWORD:-myapp_pass}
      - DB_HOST=mysql
      - DB_PORT=3306
      # Optional Kafka params if the app ever needs them
      - KAFKA_BROKERS=${KAFKA_BROKERS}
      - KAFKA_TOPIC=${KAFKA_TOPIC}
      # Internal base URL (used by cron jobs)
      - ORCH_INTERNAL_BASE_URL=http://orchestrator:8080
    volumes:
      - ./orchestrator/application.yml:/config/application.yml:ro
    ports:
      - "${ORCH_PORT:-8080}:8080" # user-facing (optional for local dev)
    networks:
      - lc_net
    depends_on:
      db:
        condition: service_healthy
      # kafka:
      #   condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -fsS http://localhost:8080/actuator/health | grep 'UP' >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 20s
    restart: unless-stopped

  # kafka-manager:
  #   image: hlebalbau/kafka-manager:stable
  #   platform: linux/amd64
  #   container_name: lc_kafka_manager
  #   environment:
  #     ZK_HOSTS: zookeeper:2181
  #     APPLICATION_SECRET: change_me_secret
  #     KM_ARGS: -Dpidfile.path=/dev/null
  #   networks:
  #     - lc_net
  #   depends_on:
  #     zookeeper:
  #       condition: service_healthy
  #     kafka:
  #       condition: service_healthy
  #   ports:
  #     - "${KAFKA_MANAGER_PORT:-9000}:9000"
  #   healthcheck:
  #     test: ["CMD", "bash", "-lc", "wget -qO- http://localhost:9000 | grep -i kafka >/dev/null"]
  #     interval: 15s
  #     timeout: 10s
  #     retries: 20
  #     start_period: 20s
  #   restart: unless-stopped

  # kafka-manager-init:
  #   image: alpine:3.20
  #   container_name: lc_kafka_manager_init
  #   environment:
  #     ZK_HOSTS: zookeeper:2181
  #     CLUSTER_NAME: logwise
  #     KAFKA_MANAGER_HOST: kafka-manager
  #     KAFKA_MANAGER_PORT: 9000
  #     KAFKA_VERSION: 2.4.0
  #     KAFKA_HOST: kafka
  #     KAFKA_JMX_PORT: 9999
  #   networks:
  #     - lc_net
  #   depends_on:
  #     kafka-manager:
  #       condition: service_started
  #     kafka:
  #       condition: service_healthy
  #     zookeeper:
  #       condition: service_healthy
  #   volumes:
  #     - ./kafka-manager/init-cluster.sh:/init-cluster.sh:ro
  #   entrypoint: ["/bin/sh", "-c", "apk add --no-cache curl netcat-openbsd && /init-cluster.sh"]
  #   restart: "on-failure"

  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: lc_spark_master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_OPTS=-Dspark.master.rest.enabled=true
    command: ["bash", "-lc", 
      "mkdir -p /opt/spark/logs && /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master --port 7077 --webui-port 8080"]
    expose:
      - "7077"
      - "8080"
      - "6066"
    ports:
      - "${SPARK_MASTER_UI_PORT:-18080}:8080"
      - 6066:6066
    volumes:
      - ../spark/target/logwise-spark-2.0.4-SNAPSHOT.jar:/opt/app/app.jar:ro
      - ./log4j.properties:/opt/spark/conf/log4j.properties:ro
      - spark_master_logs:/opt/spark/logs
    networks:
      - lc_net
    depends_on:
      kafka:
        condition: service_healthy
    # healthcheck:
    #   test: ["CMD", "bash", "-lc", ": </dev/tcp/127.0.0.1/7077"]
    #   interval: 10s
    #   timeout: 5s
    #   retries: 20
    #   start_period: 10s
    restart: unless-stopped
    mem_limit: 1g
    cpus: "1.00"

  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: lc_spark_worker
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    command: ["bash", "-lc", 
      "mkdir -p /opt/spark/logs && SPARK_WORKER_OPTS='-Dspark.worker.memory=2048m' /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8081 --memory 2048m --cores 2"]
    networks:
      - lc_net
    volumes:
      - ../spark/target/logwise-spark-2.0.4-SNAPSHOT.jar:/opt/app/app.jar:ro
      # - ./spark-kafka-vector:/opt/spark/work-dir
      - ./log4j.properties:/opt/spark/conf/log4j.properties:ro
      - spark_worker_logs:/opt/spark/logs
      # - ./spark-work-dir:/opt/spark/work
    depends_on:
      spark-master:
        condition: service_started
    ports: 
      - "8081:8081"
    healthcheck:
      test: ["CMD", "bash", "-lc", "wget -qO- http://localhost:8081 >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 10s
    restart: unless-stopped
    mem_limit: 3g
    cpus: "2.00"

  # spark-worker-2:
  #   build:
  #     context: ./spark
  #     dockerfile: Dockerfile
  #   container_name: lc_spark_worker_2
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_WORKER_MEMORY=2g
  #     - SPARK_WORKER_CORES=2
  #   command: ["bash", "-lc", 
  #     "mkdir -p /opt/spark/logs && SPARK_WORKER_OPTS='-Dspark.worker.memory=2048m' /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8081 --memory 2048m --cores 2"]
  #   networks:
  #     - lc_net
  #   volumes:
  #     - ./d11-log-management-spark/central-log-management-spark-2.0.4-SNAPSHOT.jar:/opt/app/app.jar:ro
  #     # - ./spark-kafka-vector:/opt/spark/work-dir
  #     - ./log4j.properties:/opt/spark/conf/log4j.properties:ro
  #     - spark_worker_logs:/opt/spark/logs
  #     # - ./spark-work-dir:/opt/spark/work
  #   depends_on:
  #     spark-master:
  #       condition: service_started
  #   ports: 
  #     - "8081:8081"
  #   healthcheck:
  #     test: ["CMD", "bash", "-lc", "wget -qO- http://localhost:8081 >/dev/null"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 20
  #     start_period: 10s
  #   restart: unless-stopped
  #   mem_limit: 3g
  #   cpus: "2.00"
  # spark-build:
  #   image: maven:3-eclipse-temurin-17
  #   container_name: lc_spark_build
  #   working_dir: /workspace
  #   volumes:
  #     - ./d11-log-managament-spark:/workspace
  #   command: ["bash", "-lc", "mvn -B -T 4 clean package -Dmaven.test.skip -Dmaven.artifact.threads=10 -Paws && cd target && set -e; JAR=$(ls -1 *.jar | grep -v sources | head -n1); ln -sf \"$JAR\" app.jar; echo Built $JAR and linked app.jar"]
  #   networks:
  #     - lc_net
  #   restart: "no"

  spark-client:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: lc_spark_client
    env_file:
      - .env
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - KAFKA_BROKERS=${KAFKA_BROKERS}
      - KAFKA_TOPIC=${KAFKA_TOPIC}
      - S3_BUCKET=${S3_BUCKET}
      - S3_PREFIX=${S3_PREFIX}
      - CHECKPOINT_DIR=/opt/checkpoints
      - SPARK_STREAMING=${SPARK_STREAMING}
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN:-}
      - SPARK_VERSION_MATCH=${SPARK_VERSION_MATCH}
      - HADOOP_AWS_VERSION=${HADOOP_AWS_VERSION}
      - AWS_SDK_VERSION=${AWS_SDK_VERSION}
      - APP_JAR=/opt/app/app.jar
      - APP_RESOURCE=file:/opt/app/app.jar
      - MAIN_CLASS=${MAIN_CLASS:-com.dream11.MainApplication}
      # REST API submission args - can be overridden via APP_ARGS env var
      # Note: Values with colons (like URLs) must be quoted for HOCON parsing
      # Format matches the working curl command exactly
      # Using single quotes in YAML to preserve double quotes in the value
      # Match the working curl command exactly
      # Note: kafka.cluster.dns includes port (kafka:9092) as per working curl
      # Note: kafka.manager.host should be http://kafka-manager:9000 (not 9092)
      - APP_ARGS=${APP_ARGS:-'kafka.cluster.dns="kafka",kafka.cluster.name=central-log-management-v3,kafka.manager.host="http://kafka-manager:9000",kafka.maxRatePerPartition=4000,kafka.startingOffsets=latest,kafka.subscribePattern="logs.*",spark.master.host="http://spark-master:8080"'}
      # REST API configuration
      - SPARK_REST_URL=http://spark-master:6066/v1/submissions/create
      - CLIENT_SPARK_VERSION=3.1.2
      - SPARK_APP_NAME=d11-log-management
      - SPARK_CORES_MAX=1
      - SPARK_DRIVER_CORES=1
      - SPARK_DRIVER_MEMORY=512m
      - SPARK_EXECUTOR_CORES=1
      - SPARK_EXECUTOR_MEMORY=512m
      - SPARK_DRIVER_SUPERVISE=true
      - SPARK_DRIVER_OPTS=-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties
      - SPARK_EXECUTOR_OPTS=-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties
      - SPARK_DRIVER_MAX_RESULT_SIZE=1G
      # Tenant configuration
      - TENANT_HEADER=X-Tenant-Name
      - TENANT_VALUE=${TENANT_VALUE:-D11-Prod-AWS}
      # Auto-submit via REST API on container start
      - AUTO_SPARK_REST_SUBMIT=true
    volumes:
      - ./spark/rest-submit.sh:/opt/rest-submit.sh:ro
      - ./spark/entrypoint.sh:/opt/spark-entrypoint.sh:ro
      - ./d11-log-management-spark/central-log-management-spark-2.0.4-SNAPSHOT.jar:/opt/app/app.jar:ro
      - spark_checkpoint:/opt/checkpoints
      - spark_logs:/opt/spark/logs
    entrypoint: ["/bin/bash", "/opt/spark-entrypoint.sh"]
    networks:
      - lc_net
    depends_on:
      spark-master:
        condition: service_started
      spark-worker:
        condition: service_healthy
      kafka:
        condition: service_healthy
    restart: unless-stopped
    mem_limit: 1g
    cpus: "1.00"

  healthcheck-dummy:
    build:
      context: ./healthcheck-dummy
      dockerfile: Dockerfile
    container_name: lc_healthcheck_dummy
    environment:
      - SERVICE_NAME=healthcheck-dummy
      - ENVIRONMENT=local
      - TYPE=healthcheck
    volumes:
      - healthcheck_logs:/var/log/healthcheck
      - otel_storage:/var/lib/otelcol/storage
    networks:
      - lc_net
    depends_on:
      vector:
        condition: service_started
      kafka:
        condition: service_healthy
    restart: unless-stopped
    mem_limit: 256m
    cpus: "0.25"
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:13133"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  grafana:
    image: grafana/grafana:latest
    container_name: lc_grafana
    env_file:
      - .env
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-athena-datasource,yesoreyeram-infinity-datasource
      # Fix external URL to avoid redirects to Docker internal IP
      - GF_SERVER_DOMAIN=localhost
      - GF_SERVER_HTTP_ADDR=0.0.0.0
      - GF_SERVER_HTTP_PORT=3000
      - GF_SERVER_ROOT_URL=http://localhost:3000/
      # Use MySQL as Grafana database instead of default SQLite
      - GF_DATABASE_TYPE=mysql
      - GF_DATABASE_HOST=db:3306
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=grafana
      - GF_DATABASE_PASSWORD=grafana
      # AWS credentials for Athena datasource
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN:-}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "${GRAFANA_PORT:-3000}:3000" # only user-facing port
    networks:
      - lc_net
    depends_on:
      db:
        condition: service_healthy
      grafana-db-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "bash", "-lc", "wget -qO- http://localhost:3000/api/health | grep 'ok' >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 20s
    restart: unless-stopped

  grafana-db-init:
    image: mysql:8.0
    container_name: lc_grafana_db_init
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root_pass}
    networks:
      - lc_net
    depends_on:
      db:
        condition: service_healthy
    entrypoint: [ "sh", "-c" ]
    # Idempotent DB/user creation for Grafana
    command: >
      "mysql -h db -uroot -p\"${MYSQL_ROOT_PASSWORD:-root_pass}\" -e
      \"CREATE DATABASE IF NOT EXISTS grafana CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
      CREATE USER IF NOT EXISTS 'grafana'@'%' IDENTIFIED BY 'grafana';
      GRANT ALL PRIVILEGES ON grafana.* TO 'grafana'@'%';
      FLUSH PRIVILEGES;\""
    restart: "no"

  scheduler:
    image: alpine:3.20
    depends_on:
      orchestrator: { condition: service_started }
    environment:
      TZ: Asia/Kolkata
    volumes:
      - ./cron/crontab:/crontab:ro
    networks:
      - lc_net
    entrypoint: [ "/bin/sh","-lc" ]
    command: >
      'apk add --no-cache curl tzdata &&
       crontab /crontab &&
       crond -f -l 8'
  
